#!/bin/bash
# -*- coding: utf-8 -*-
# Author: PÃ¤r Andersson (National Supercomputer Centre, Sweden)
# Version: 0.3 2007-07-30
#
# 2011-06-23: Joerg Bornschein <bornschein@fias.uni-frankfurt.de>
#   Make this script find its own path
#   https://github.com/jbornschein/srun.x11/blob/master/srun.x11
#
# 2014-06-26: L. Shawn Matott <lsmatott@buffalo.edu>
#   fisbatch is based on srun.x11 with extensions to handle
#   centers that have multiple clusters and partitions. It also,
#   has additional logic to detect and report downtimes rather
#   than leave users waiting on resources that are not available
#   and likely won't be available for some time.
#
# This will submit a batch script that starts screen on a node.
# Then ssh is used to connect to the node and attach the screen.
# The result is very similar to an interactive shell in PBS
# (qsub -I)
#
#  FISBATCH = Friendly Interactive SBATCH
#

if [ "$1" == "--help" ]; then
  echo " "
  echo "========================================="
  echo "fisattach                                "
  echo " "
  echo "    Re-attaching fisbatch screen command."
  echo " "
  echo "   Usage:                                "
  echo "      fisbatch JobID                     "
  echo "========================================="
  echo " "
  exit
fi

# determine STUBL install location
# Copied from Apache Ant:
# https://git-wip-us.apache.org/repos/asf?p=ant.git;a=blob;f=src/script/ant;h=b5ed5be6a8fe3a08d26dea53ea0fb3f5fab45e3f
if [ -z "$STUBL_HOME" -o ! -d "$STUBL_HOME" ] ; then
  ## resolve links - $0 may be a link to stubl's home
  PRG="$0"
  progname=`basename "$0"`

  # need this for relative symlinks
  while [ -h "$PRG" ] ; do
    ls=`ls -ld "$PRG"`
    link=`expr "$ls" : '.*-> \(.*\)$'`
    if expr "$link" : '/.*' > /dev/null; then
    PRG="$link"
    else
    PRG=`dirname "$PRG"`"/$link"
    fi
  done

  STUBL_HOME=`dirname "$PRG"`/..

  # make it fully qualified
  STUBL_HOME=`cd "$STUBL_HOME" > /dev/null && pwd`
fi

# setup STUBL environment
. $STUBL_HOME/conf/stubl 

# Location of helper scripts
MYDIR=$STUBL_HOME/bin/fisbatch_helpers

# Batch Script that starts SCREEN
BS=$MYDIR/_interactive
# Interactive screen script
IS=$MYDIR/_interactive_screen

JOB=`echo $1 | awk '{ printf("%d", $1 + 0); }'`

# Make sure the job is always canceled
#trap "{ $STUBL_SCANCEL --clusters=$cluster --partition=$partition $JOB 2>/dev/null; exit; }" SIGINT SIGTERM EXIT
trap "{ $STUBL_SCANCEL $JOB 2>/dev/null; exit; }" SIGINT SIGTERM EXIT

echo "FISBATCH -- attaching for JOBID $JOB"
while true; do
    sleep 1s

    # Check job status
    #STATUS=`squeue --clusters=$cluster --partition=$partition -j $JOB -t PD,R -h -o %t | grep -v "^CLUSTER"`
    STATUS=`squeue -j $JOB -t PD,R -h -o %t | grep -v "^CLUSTER"`

    if [ "$STATUS" = "R" ];then
	# Job is running, break the while loop
        echo "!"
        sleep 3s
	break
    elif [ "$STATUS" != "PD" ];then
        echo "!"
	echo "Job is not Running or Pending. Aborting"
        #scancel --clusters=$cluster --partition=$partition $JOB 2>/dev/null
        scancel $JOB 2>/dev/null
        echo "FISBATCH -- aborting job ($JOB)"
	exit 1
    fi

    echo -n "."
done

# Determine the head node in the job:
#NODE=`srun --jobid=$JOB -N1 hostname`
HNODE=""
usr=`whoami`
if [ ${#usr} -gt "8" ]; then
  usr=`id -u $usr`
fi 
#NODE=`squeue -h --clusters=${cluster} --partition=${partition} --jobs=${JOB} -o %N | grep -v "^CLUSTER"`
NODE=`squeue -h --jobs=${JOB} -o %N | grep -v "^CLUSTER"`
NODE=`nodeset -e $NODE`
for i in $NODE; do
   screenTest=`ssh $i "ps -ef | grep "^"$usr | grep \[S\]CREEN | wc -l"`
   if [ "$screenTest" != "0" ]; then
      HNODE=$i
      break
   fi
done

if [ "$HNODE" == "" ]; then
  echo "Couldn't identify the head node - SCREEN not running on any node!"
  echo "FISBATCH -- aborting job"
  exit
fi

echo "FISBATCH -- Connecting to head node ($HNODE)"
# a brief pause is needed?
sleep 1s

# SSH to the node and attach the screen
ssh -X -t $HNODE $IS slurm$JOB

# The trap will now cancel the job before exiting.
echo "FISBATCH -- exiting job"

